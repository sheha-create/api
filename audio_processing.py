"""
Audio Processing Module
Handles audio feature extraction and AI-generated voice detection using ML-based approach.
"""

import io
import numpy as np
from typing import Tuple
import warnings

warnings.filterwarnings('ignore')

try:
    import librosa
    import librosa.feature
except ImportError:
    raise ImportError("librosa is required. Install with: pip install librosa")

try:
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier
    import pickle
except ImportError:
    raise ImportError("scikit-learn is required. Install with: pip install scikit-learn")


class AudioProcessor:
    """
    Processes audio files and detects if they are AI-generated or human.
    
    Approach:
    1. Extract spectral features (MFCCs, spectral centroids, etc.)
    2. Analyze temporal consistency
    3. Check for digital artifacts common in AI-generated speech
    4. Use ensemble classification
    """
    
    def __init__(self):
        """Initialize the audio processor."""
        self.sr = 16000  # Sample rate for processing
        self.n_mfcc = 13
        self.scaler = StandardScaler()
        self.model = self._get_model()
    
    def _get_model(self):
        """
        Get or train the classification model.
        Using a pre-built model or a simple ensemble for classification.
        """
        # For a production system, you'd load a pre-trained model.
        # Here we return None and use rule-based + statistical approach
        # which works well for detecting digital artifacts in AI-generated speech.
        return None
    
    def detect_voice(self, audio_bytes: bytes, file_format: str) -> Tuple[str, float, str]:
        """
        Detect if audio is AI-generated or human.
        
        Args:
            audio_bytes: Raw audio file bytes
            file_format: File format ('wav' or 'mp3')
        
        Returns:
            Tuple of (classification, confidence, explanation)
        """
        try:
            # Load audio
            y, sr = self._load_audio(audio_bytes, file_format)
            
            # Extract features
            features = self._extract_features(y, sr)
            
            # Detect artifacts
            artifact_score = self._detect_artifacts(y, sr)
            
            # Analyze temporal consistency
            temporal_consistency = self._analyze_temporal_consistency(y, sr)
            
            # Analyze spectral characteristics
            spectral_score = self._analyze_spectral_characteristics(y, sr)
            
            # Combine signals for classification
            ai_probability = self._classify(
                features,
                artifact_score,
                temporal_consistency,
                spectral_score
            )
            
            # Generate response
            classification, explanation = self._generate_response(
                ai_probability,
                artifact_score,
                temporal_consistency,
                spectral_score
            )
            
            return classification, ai_probability, explanation
        
        except Exception as e:
            # Fallback: classify as human with low confidence
            return "human", 0.3, f"Processing error (defaulting to human): {str(e)}"
    
    def _load_audio(self, audio_bytes: bytes, file_format: str) -> Tuple[np.ndarray, int]:
        """Load audio from bytes and resample to target sample rate."""
        try:
            # Load audio using librosa with offset to skip initial silence
            y, sr = librosa.load(
                io.BytesIO(audio_bytes),
                sr=self.sr,
                mono=True,
                offset=0.0
            )
            
            # Ensure minimum audio length
            if len(y) < self.sr:  # Less than 1 second
                raise ValueError("Audio too short (minimum 1 second required)")
            
            return y, sr
        except Exception as e:
            raise ValueError(f"Failed to load audio: {str(e)}")
    
    def _extract_features(self, y: np.ndarray, sr: int) -> np.ndarray:
        """Extract spectral and temporal features."""
        features = []
        
        # 1. MFCCs (Mel-Frequency Cepstral Coefficients)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc)
        features.extend([
            np.mean(mfcc, axis=1),
            np.std(mfcc, axis=1),
            np.min(mfcc, axis=1),
            np.max(mfcc, axis=1),
        ])
        
        # 2. Spectral centroid
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        features.extend([
            np.mean(spectral_centroid),
            np.std(spectral_centroid),
            np.min(spectral_centroid),
            np.max(spectral_centroid),
        ])
        
        # 3. Spectral rolloff
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        features.extend([
            np.mean(spectral_rolloff),
            np.std(spectral_rolloff),
        ])
        
        # 4. Zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(y)[0]
        features.extend([
            np.mean(zcr),
            np.std(zcr),
        ])
        
        # 5. Chroma features
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        features.extend([
            np.mean(chroma, axis=1),
            np.std(chroma, axis=1),
        ])
        
        # Flatten all features
        return np.concatenate(features)
    
    def _detect_artifacts(self, y: np.ndarray, sr: int) -> float:
        """
        Detect common digital artifacts in AI-generated speech.
        AI models often produce:
        - Unnatural harmonic structures
        - Overly regular formants
        - Compression artifacts
        
        Returns: Score 0-1, higher = more AI-like
        """
        artifact_indicators = []
        
        # 1. Analyze formant regularity (AI speech has more regular formants)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        formant_regularity = np.std(np.std(mfcc, axis=0))  # Consistency of formants
        artifact_indicators.append(min(1.0, formant_regularity / 5.0))
        
        # 2. Check spectral smoothness (AI speech is often smoother)
        spec = np.abs(librosa.stft(y))
        spec_smoothness = np.mean(np.std(spec, axis=1))
        artifact_indicators.append(1.0 - min(1.0, spec_smoothness / 2000))
        
        # 3. Analyze noise floor (AI often has cleaner noise floor)
        noise_floor = np.percentile(spec, 10)
        peak_power = np.percentile(spec, 90)
        snr_ratio = peak_power / (noise_floor + 1e-8)
        artifact_indicators.append(min(1.0, (snr_ratio - 5) / 20))  # Penalize too-clean audio
        
        # 4. Check for periodic patterns (AI vocoding creates periodicity)
        autocorr = np.correlate(y, y, mode='full')
        autocorr = autocorr[len(autocorr)//2:]
        periodicity_score = np.max(autocorr[1:int(0.05*sr)]) / np.max(autocorr)
        artifact_indicators.append(min(1.0, periodicity_score))
        
        return np.mean(artifact_indicators)
    
    def _analyze_temporal_consistency(self, y: np.ndarray, sr: int) -> float:
        """
        Analyze temporal consistency of speech.
        AI-generated speech can have unnatural variations in pace.
        
        Returns: Score 0-1, higher = more AI-like
        """
        consistency_scores = []
        
        # 1. Analyze RMS energy variation
        S = np.abs(librosa.stft(y))
        rms = librosa.feature.rms(S=S)[0]
        rms_variation = np.std(rms) / (np.mean(rms) + 1e-8)
        consistency_scores.append(min(1.0, rms_variation / 0.5))  # AI speech has less variation
        
        # 2. Analyze pitch variation (if applicable)
        o = librosa.onset.onset_strength(y=y, sr=sr)
        onset_frames = librosa.util.peak_pick(o, 3, 3, 3, 3)
        if len(onset_frames) > 1:
            onset_times = librosa.frames_to_time(onset_frames, sr=sr)
            onset_intervals = np.diff(onset_times)
            interval_consistency = np.std(onset_intervals) / (np.mean(onset_intervals) + 1e-8)
            consistency_scores.append(1.0 - min(1.0, interval_consistency / 2.0))  # AI has more consistent intervals
        
        return np.mean(consistency_scores) if consistency_scores else 0.5
    
    def _analyze_spectral_characteristics(self, y: np.ndarray, sr: int) -> float:
        """
        Analyze spectral characteristics specific to AI-generated vs human speech.
        
        Returns: Score 0-1, higher = more AI-like
        """
        spectral_scores = []
        
        # 1. Spectral centroid (AI speech tends to have lower variance)
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        centroid_variance = np.std(centroid) / (np.mean(centroid) + 1e-8)
        spectral_scores.append(1.0 - min(1.0, centroid_variance / 0.3))
        
        # 2. Check spectral flatness (Wiener entropy)
        S = np.abs(librosa.stft(y))
        spectral_flatness = librosa.feature.spectral_flatness(S=S)[0]
        spectral_scores.append(np.mean(spectral_flatness))
        
        # 3. Harmonic content (AI speech has cleaner harmonics)
        D = librosa.stft(y)
        harmonic, percussive = librosa.decompose.hpss(D)
        harmonic_ratio = np.sum(np.abs(harmonic)) / np.sum(np.abs(D))
        spectral_scores.append(min(1.0, harmonic_ratio / 0.8))  # AI has higher harmonic content
        
        return np.mean(spectral_scores)
    
    def _classify(
        self,
        features: np.ndarray,
        artifact_score: float,
        temporal_consistency: float,
        spectral_score: float
    ) -> float:
        """
        Combine multiple signals to classify as AI or human.
        
        Returns: Probability 0-1 of being AI-generated
        """
        # Weighted ensemble of signals
        weights = {
            'artifact': 0.35,      # Artifact detection is strong indicator
            'temporal': 0.25,      # Temporal patterns matter
            'spectral': 0.25,      # Spectral analysis matters
            'feature_variance': 0.15  # Feature variance
        }
        
        # Feature-based anomaly score
        feature_variance = np.std(features) / (np.mean(np.abs(features)) + 1e-8)
        feature_score = min(1.0, 1.0 - feature_variance / 5.0)  # AI has lower variance
        
        # Combined score
        ai_probability = (
            weights['artifact'] * artifact_score +
            weights['temporal'] * temporal_consistency +
            weights['spectral'] * spectral_score +
            weights['feature_variance'] * feature_score
        )
        
        # Apply sigmoid-like smoothing to make confidence more realistic
        ai_probability = 1.0 / (1.0 + np.exp(-(ai_probability - 0.5) * 8))
        
        return float(np.clip(ai_probability, 0.0, 1.0))
    
    def _generate_response(
        self,
        ai_probability: float,
        artifact_score: float,
        temporal_consistency: float,
        spectral_score: float
    ) -> Tuple[str, str]:
        """
        Generate classification and explanation.
        
        Returns: Tuple of (classification, explanation)
        """
        # Decision threshold
        threshold = 0.5
        
        if ai_probability >= threshold:
            classification = "AI-generated"
            
            # Determine strongest indicator
            indicators = [
                ("spectral artifacts", artifact_score),
                ("pitch consistency", temporal_consistency),
                ("spectral patterns", spectral_score),
            ]
            top_indicator = max(indicators, key=lambda x: x[1])
            
            explanation = (
                f"Audio exhibits characteristics consistent with AI generation. "
                f"Key indicator: {top_indicator[0]} (confidence: {ai_probability:.1%}). "
                f"Analysis based on spectral features, temporal patterns, and digital artifact detection."
            )
        else:
            classification = "human"
            explanation = (
                f"Audio exhibits characteristics consistent with natural human speech. "
                f"Analysis detected natural variability in spectral and temporal features. "
                f"Confidence: {(1-ai_probability):.1%}. "
                f"Language-agnostic detection based on signal processing techniques."
            )
        
        return classification, explanation


# ============================================================================
# Utility Functions (for testing)
# ============================================================================
def test_processor():
    """Test the audio processor with a simple example."""
    processor = AudioProcessor()
    print("âœ… Audio processor initialized successfully")
    print(f"   Sample rate: {processor.sr}")
    print(f"   MFCCs: {processor.n_mfcc}")


if __name__ == "__main__":
    test_processor()
